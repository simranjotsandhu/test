import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load LLaMA model and tokenizer from Hugging Face
model_name = "meta/llama-3.2b"  # Replace with the correct model path or identifier
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Function to generate SQL query
def generate_sql(nl_query: str, schema: dict) -> str:
    # Convert schema into a string that the model can understand
    schema_str = ""
    for table, columns in schema.items():
        schema_str += f"Table {table} has columns: {', '.join(columns)}.\n"

    # Combine the natural language query with the schema
    input_text = f"Given the schema:\n{schema_str}\nTranslate the following natural language query into SQL:\n{nl_query}\nSQL Query:"

    # Tokenize the input
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate the SQL query (with max length constraint)
    outputs = model.generate(inputs["input_ids"], max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)

    # Decode the output
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Strip the leading prompt to get only the generated SQL
    return sql_query.split('SQL Query:')[-1].strip()

# Example schema
schema = {
    "employees": ["id", "name", "position", "salary"],
    "departments": ["id", "department_name", "location"],
}

# Example natural language query
nl_query = "What are the names of all employees in the HR department?"

# Generate the SQL query
sql_query = generate_sql(nl_query, schema)
print("Generated SQL Query:", sql_query)











import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load LLaMA model and tokenizer from Hugging Face
model_name = "meta/llama-3.2b"  # Replace with the correct model path or identifier
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Check if CUDA (GPU) is available and set device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Function to generate SQL query
def generate_sql(nl_query: str, schema_str: str) -> str:
    # Combine the natural language query with the schema string
    input_text = f"Given the schema:\n{schema_str}\nTranslate the following natural language query into SQL:\n{nl_query}\nSQL Query:"

    # Tokenize the input and move input tensors to the same device as the model
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    # Generate the SQL query (with max length constraint)
    outputs = model.generate(inputs["input_ids"], max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)

    # Decode the output
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Strip the leading prompt to get only the generated SQL
    return sql_query.split('SQL Query:')[-1].strip()

# Example schema as a string
schema_str = """
Table employees has columns: id, name, position, salary.
Table departments has columns: id, department_name, location.
"""

# Example natural language query
nl_query = "What are the names of all employees in the HR department?"

# Generate the SQL query
sql_query = generate_sql(nl_query, schema_str)
print("Generated SQL Query:", sql_

######


import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load LLaMA model and tokenizer from Hugging Face
model_name = "meta/llama-3.2b"  # Replace with the correct model path or identifier
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Check if CUDA (GPU) is available and set device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Function to generate SQL query
def generate_sql(nl_query: str, schema_str: str) -> str:
    # Combine the natural language query with the schema string
    input_text = f"Given the schema:\n{schema_str}\nTranslate the following natural language query into SQL:\n{nl_query}\nSQL Query:"

    # Tokenize the input and move input tensors to the same device as the model
    inputs = tokenizer(input_text, return_tensors="pt").to(device)  # Move tokenized inputs to the correct device

    # Generate the SQL query (with max length constraint)
    outputs = model.generate(inputs["input_ids"], max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)

    # Decode the output
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Strip the leading prompt to get only the generated SQL
    return sql_query.split('SQL Query:')[-1].strip()

# Example schema as a string
schema_str = """
Table employees has columns: id, name, position, salary.
Table departments has columns: id, department_name, location.
"""

# Example natural language query
nl_query = "What are the names of all employees in the HR department?"

# Generate the SQL query
sql_query = generate_sql(nl_query, schema_str)
print("Generated SQL Query:", sql_query)



